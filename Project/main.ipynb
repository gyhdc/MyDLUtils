{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from utils import datasets\n",
    "from utils import utils\n",
    "from utils import train_val\n",
    "from utils import net #网络文件于此\n",
    "from utils import metrics\n",
    "import warnings\n",
    "# 完全禁用警告\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import Config,Logs,BestSelector\n",
    "config=utils.Config(\n",
    "    dataset_sep=[\n",
    "        0.82,0.17,0.01          \n",
    "        ],\n",
    "    resize_size=128,#图像尺寸\n",
    "    batch_size=128,\n",
    "    lr=0.0007,\n",
    "    epochs=200,#epoch轮数\n",
    "    hidden_size=256,\n",
    "    optim=\"Adam\",\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-4,\n",
    "    seed=42,\n",
    "    mean= [0.50638 ,0.49962538 ,0.45205265],\n",
    "    std=[0.23568255 ,0.24141274 ,0.25167742],\n",
    "    AMP=True,\n",
    "    checkpoint_interval=0.25,#只保存4个模型\n",
    "    source_dir=r\"Cifar-10\",#原始数据集，每个分类一个文件夹，每个文件夹里包含多个图片\n",
    "    data_path=r\"data\\Cifar-10\",#项目数据集\n",
    "    # classes=[\"Apple\",\"Carambola\",\"Pear\",\"Plum\",\"Tomatoes\"],\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Cifar-10 数据集已存在，无需重新划分\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(config.seed)\n",
    "isSplit= (not datasets.check_data_exist(config.data_path))#数据集不存在，则从原始数据存放处，转移数据集\n",
    "if isSplit:\n",
    "    print(f\"{config.data_path} 数据集不存在，将从source_dir:{config.source_dir}中获取数据\")\n",
    "    print(f\"清理源文件夹:{config.data_path}\")\n",
    "    datasets.clear_folder(config.data_path)\n",
    "    for dir in tqdm(config.classes,desc=\"处理原始数据：\"):\n",
    "        source_dir=os.path.join(config.source_dir, dir)\n",
    "        print(source_dir)\n",
    "        datasets.split_data(source_dir,target_dir=config.data_path,label=dir,sep=config.dataset_sep)\n",
    "else:\n",
    "    print(f\"{config.data_path} 数据集已存在，无需重新划分\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 本地数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data_path(typ,path=config.data_path):\n",
    "#     return os.path.join(path,typ)\n",
    "# train_dataset=datasets.CustomImageDataset(\n",
    "#     get_data_path(\"train\"),#data/train\n",
    "#     classes=config.classes,\n",
    "#     transform=datasets.get_transform(\n",
    "#         resize_size=config.resize_size,\n",
    "#         mean=config.mean,std=config.std\n",
    "#         )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载CIFAR-10数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "原始训练集大小：37500\n",
      "原始验证集大小：12500\n",
      "原始测试集大小：10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torchvision\n",
    "train_val_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data/Cifar-10', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=datasets.get_transform(\n",
    "        chance=\"train\",\n",
    "        resize_size=config.resize_size,\n",
    "        mean=(0.5, 0.5, 0.5), \n",
    "        std=(0.5, 0.5, 0.5)\n",
    "    )\n",
    ")\n",
    "train_dataset,val_dataset=datasets.get_parts_of_datasets(train_val_dataset,rate=0.75,only_train=False)#训练验证3-1开\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data/Cifar-10', train=False, download=True, transform=datasets.get_transform(\n",
    "    chance=\"val\",\n",
    "    resize_size=config.resize_size,\n",
    "    mean=(0.5, 0.5, 0.5), \n",
    "    std=(0.5, 0.5, 0.5)\n",
    "))\n",
    "print(f\"原始训练集大小：{len(train_dataset)}\")\n",
    "print(f\"原始验证集大小：{len(val_dataset)}\")\n",
    "print(f\"原始测试集大小：{len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load数据集，并根据数据量进行裁剪。CIFAR-10较大，本身为学习项目只取其中1/4数据进行训练测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本次训练用训练集大小：7500\n",
      "本次训练用验证集大小：2500\n",
      "本次测试用测试集大小：2000\n",
      "tarin_dataloader加载完毕, 58个batch, batch大小为128\n",
      "val_dataloader  加载完毕, 20个batch, batch大小为128\n",
      "test_dataloader 加载完毕, 16个batch, batch大小为128\n"
     ]
    }
   ],
   "source": [
    "crop_rate=0.20\n",
    "train_dataset_crop=datasets.get_parts_of_datasets(train_dataset,crop_rate)\n",
    "val_dataset_crop=datasets.get_parts_of_datasets(val_dataset,crop_rate)\n",
    "test_dataset_crop=datasets.get_parts_of_datasets(test_dataset,crop_rate)\n",
    "train_dataset_size=len(train_dataset_crop)\n",
    "val_dataset_size=len(val_dataset_crop)\n",
    "test_dataset_size=len(test_dataset_crop)\n",
    "\n",
    "print(f\"本次训练用训练集大小：{len(train_dataset_crop)}\")\n",
    "print(f\"本次训练用验证集大小：{len(val_dataset_crop)}\")\n",
    "print(f\"本次测试用测试集大小：{len(test_dataset_crop)}\")\n",
    "\n",
    "train_loader=DataLoader(train_dataset_crop,batch_size=config.batch_size,shuffle=True,drop_last=True)\n",
    "val_loader=DataLoader(val_dataset_crop,batch_size=config.batch_size,shuffle=False)\n",
    "test_loader=DataLoader(test_dataset_crop,batch_size=config.batch_size,shuffle=False)\n",
    "\n",
    "print(f\"tarin_dataloader加载完毕, {len(train_loader)}个batch, batch大小为{config.batch_size}\")\n",
    "print(f\"val_dataloader  加载完毕, {len(val_loader)}个batch, batch大小为{config.batch_size}\")\n",
    "print(f\"test_dataloader 加载完毕, {len(test_loader)}个batch, batch大小为{config.batch_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 记录本次训练和测试用的数据量，还有数据样本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 128, 128])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "#查看数据加载情况\n",
    "for inputs, labels in train_loader:\n",
    "    inputs=inputs.to(config.device)\n",
    "    labels=labels.to(config.device)\n",
    "    print(inputs.shape)\n",
    "    print(labels.shape)\n",
    "    config.update(\n",
    "        inputs_shape=inputs.shape\n",
    "        )\n",
    "    break\n",
    "config.update(\n",
    "    train_datasize=train_dataset_size,\n",
    "    val_datasetsize=val_dataset_size,\n",
    "    test_datasetsize=test_dataset_size,\n",
    "    datasets_crop_rate=crop_rate,\n",
    "    classes=train_val_dataset.classes,#原始数据集保留classes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数数量：3217226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm up ....: 100%|██████████| 100/100 [00:05<00:00, 17.23it/s]\n",
      "Testing ...: 100%|██████████| 300/300 [00:07<00:00, 41.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "推理一个batch的时间：0.02365310131708781 s\n",
      "inference_time:0.02365310131708781 s\n",
      "parameters_num:3217226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dataset_sep : [0.82, 0.17, 0.01] \n",
       "resize_size : 128 \n",
       "batch_size : 128 \n",
       "lr : 0.0007 \n",
       "epochs : 200 \n",
       "hidden_size : 256 \n",
       "optim : Adam \n",
       "momentum : 0.9 \n",
       "weight_decay : 0.0001 \n",
       "seed : 42 \n",
       "mean : [0.50638, 0.49962538, 0.45205265] \n",
       "std : [0.23568255, 0.24141274, 0.25167742] \n",
       "AMP : True \n",
       "checkpoint_interval : 0.25 \n",
       "source_dir : Cifar-10 \n",
       "data_path : data\\Cifar-10 \n",
       "device : cuda \n",
       "inputs_shape : torch.Size([128, 3, 128, 128]) \n",
       "train_datasize : 7500 \n",
       "val_datasetsize : 2500 \n",
       "test_datasetsize : 2000 \n",
       "datasets_crop_rate : 0.2 \n",
       "classes : ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'] \n",
       "network : MobileNetV1 \n",
       "inference_time : 0.02365310131708781 \n",
       "parameters_num : 3217226 "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_pretrained(config=config):\n",
    "    '''\n",
    "        获取预训练模型\n",
    "        @param config: 配置文件\n",
    "        @return: 预训练模型\n",
    "    '''\n",
    "    # model=net.BinaryClassificationMobileNetV3Large(out_size=len(config.classes))\n",
    "    # model=net.AutoCNN(\n",
    "    #     input_channels=3,\n",
    "    #     num_classes=len(config.classes),\n",
    "    #     input_size=config.inputs_shape[2:],\n",
    "    #     # hidden_channels_size_1=32,\n",
    "    #     # hidden_channels_size_2=64,\n",
    "    #     # hidden_channels_size_3=128,\n",
    "    #     # mlp_hidden_size=256\n",
    "    # )\n",
    "    # model=net.ResNet(\n",
    "    #     num_classes=len(config.classes),\n",
    "    #     input_channels=config.inputs_shape[1],\n",
    "    #)#最后全局池化层压下了尺寸，不需要提供输入尺寸\n",
    "    # model=net.AlexNet(\n",
    "    #     num_classes=len(config.classes),\n",
    "    # )\n",
    "    # model=net.VGGNet(\n",
    "    #     input_channels=config.inputs_shape[1],\n",
    "    #     num_classes=len(config.classes),\n",
    "    #     config=\"D\",\n",
    "    #     classifier_hidden_size=[4096,1024]\n",
    "    # )\n",
    "    # model=net.GoogLeNet(\n",
    "    #     input_channels=3,\n",
    "    #     num_classes=len(config.classes),\n",
    "    #     AAP_shape=(4,4),\n",
    "    #     aux_classify=True\n",
    "    # )\n",
    "    model=net.MobileNetV1(\n",
    "        num_classes=len(config.classes),\n",
    "        gamma=1\n",
    "    )\n",
    "    return model.to(config.device)\n",
    "\n",
    "test_model=get_pretrained()\n",
    "measurer=metrics.ModelMeasurer(test_model)\n",
    "unit=1\n",
    "parameters_num,inference_time=measurer.simply_check_model(input_shape=config.inputs_shape)\n",
    "print(f\"inference_time:{inference_time} s\")\n",
    "print(f\"parameters_num:{parameters_num}\")\n",
    "config.update(\n",
    "    network=test_model.__class__.__name__,\n",
    "    inference_time=inference_time,\n",
    "    parameters_num=parameters_num,\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   0%|          | 1/200 [00:11<37:24, 11.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, train_Loss: 2.1464, val_Loss: 2.1168 ,Val Accuracy: 0.2072, Precision: 0.2251, Recall: 0.2077\n",
      "当前最好的模型：acc : 0.2072 ,loss : 2.1464 ,precision : 0.2251 ,recall : 0.2077 ,ap : 0.1786 ,epoch : 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   0%|          | 1/200 [00:14<48:39, 14.67s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlr, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m bestMod,train_logs\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbestMod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbestMod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_logs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcheckpoint_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshow_progress_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43mAMP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAMP\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#是否使用混合精度训练\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmulti_loss_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#动态调整学习率\u001b[39;49;00m\n\u001b[0;32m     26\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# num_epochs=config.epochs\u001b[39;49;00m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Desktop\\DeepLearning\\12.XMU-Review\\MyDLUtils\\Project\\utils\\train_val.py:249\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, train_loader, val_loader, bestMod, train_logs, config, metrics_weights, num_epochs, checkpoint_interval, show_progress_interval, AMP, multi_loss_weight, lr_scheduler_step)\u001b[0m\n\u001b[0;32m    246\u001b[0m     lr_scheduler \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39mlr_scheduler_step, patience\u001b[38;5;241m=\u001b[39mfinal_checkpoint_interval, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_epochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;66;03m# 训练阶段\u001b[39;00m\n\u001b[1;32m--> 249\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmulti_loss_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_loss_weight\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;66;03m# 验证阶段\u001b[39;00m\n\u001b[0;32m    258\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m validate_model(\n\u001b[0;32m    259\u001b[0m         model, \n\u001b[0;32m    260\u001b[0m         val_loader, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m         criterion\u001b[38;5;241m=\u001b[39mcriterion\n\u001b[0;32m    264\u001b[0m     )\n",
      "File \u001b[1;32md:\\Desktop\\DeepLearning\\12.XMU-Review\\MyDLUtils\\Project\\utils\\train_val.py:39\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, criterion, optimizer, train_loader, device, scaler, multi_loss_weight)\u001b[0m\n\u001b[0;32m     36\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     37\u001b[0m use_amp \u001b[38;5;241m=\u001b[39m scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     40\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     41\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\mmdetection\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\mmdetection\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\mmdetection\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\mmdetection\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\mmdetection\\lib\\site-packages\\torch\\utils\\data\\dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[1;32m--> 298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\mmdetection\\lib\\site-packages\\torch\\utils\\data\\dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[1;32m--> 298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\mmdetection\\lib\\site-packages\\torchvision\\datasets\\cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    115\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\mmdetection\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\mmdetection\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\mmdetection\\lib\\site-packages\\torchvision\\transforms\\transforms.py:979\u001b[0m, in \u001b[0;36mRandomResizedCrop.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    972\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be cropped and resized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Randomly cropped and resized image.\u001b[39;00m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 979\u001b[0m     i, j, h, w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mresized_crop(img, i, j, h, w, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolation, antialias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mantialias)\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\mmdetection\\lib\\site-packages\\torchvision\\transforms\\transforms.py:943\u001b[0m, in \u001b[0;36mRandomResizedCrop.get_params\u001b[1;34m(img, scale, ratio)\u001b[0m\n\u001b[0;32m    940\u001b[0m _, height, width \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mget_dimensions(img)\n\u001b[0;32m    941\u001b[0m area \u001b[38;5;241m=\u001b[39m height \u001b[38;5;241m*\u001b[39m width\n\u001b[1;32m--> 943\u001b[0m log_ratio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mratio\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m    945\u001b[0m     target_area \u001b[38;5;241m=\u001b[39m area \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39muniform_(scale[\u001b[38;5;241m0\u001b[39m], scale[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "bestMod=utils.BestSelector(acc=0)\n",
    "train_logs=utils.Logs()\n",
    "model=get_pretrained()\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.BCELoss()\n",
    "if config.optim == \"Adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "else:\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config.lr, momentum=0.9, weight_decay=1e-4)\n",
    "    \n",
    "bestMod,train_logs=train_val.train_model(\n",
    "            model, \n",
    "            criterion,\n",
    "            optimizer, \n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            bestMod=bestMod,\n",
    "            train_logs=train_logs,\n",
    "            config=config, \n",
    "            checkpoint_interval=10000,\n",
    "            show_progress_interval=3,\n",
    "            AMP=config.AMP,#是否使用混合精度训练\n",
    "            multi_loss_weight=[1,0.3,0.3],\n",
    "            lr_scheduler_step=0.7,#动态调整学习率\n",
    "            # num_epochs=config.epochs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 epoch中 最好的模型\n",
      "acc : 0.5748 ,loss : 1.0468 ,precision : 0.5797 ,recall : 0.5726 ,ap : 0.0915 ,epoch : 27 \n"
     ]
    }
   ],
   "source": [
    "print(f\"{config.epochs} epoch中 最好的模型\")\n",
    "print(bestMod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存模型超参数和训练日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_weights\\MobileNetV-acc=0.5748-loss=1.047-max_epochs=200\n",
      "{'acc': 0.5748, 'model': 'save_weights\\\\MobileNetV-acc=0.5748-loss=1.047-max_epochs=200\\\\best.pth', 'loss': 1.046791520611993, 'precision': 0.5797411973688208, 'recall': 0.5725587596346459, 'ap': 0.09154744311197122, 'epoch': 27, 'checkpoints': {'checkpoint_0': 'save_weights\\\\MobileNetV-acc=0.5748-loss=1.047-max_epochs=200\\\\checkpoint_0.pth'}}\n"
     ]
    }
   ],
   "source": [
    "saveDir=r'save_weights'\n",
    "saveDir=os.path.join(\n",
    "        saveDir,\n",
    "        f'{bestMod.model.__class__.__name__[:10]}-acc={round(bestMod.acc,5)}-loss={round(bestMod.loss,3)}-max_epochs={config.epochs}'\n",
    ")  \n",
    "utils.saveProcess(\n",
    "    saveDir=saveDir,\n",
    "    bestMod=bestMod,\n",
    "    train_log=train_logs,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "模型测试中:: 100%|██████████| 16/16 [00:01<00:00, 11.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.6255,\n",
       " 'Precision': 0.6261924414212041,\n",
       " 'Recall': 0.6238458165625979,\n",
       " 'F1': 0.6176151310374951,\n",
       " 'AP': 0.0799395743929934,\n",
       " 'Loss': 1.109235592186451}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from utils import utils,train_val\n",
    "import os\n",
    "\n",
    "# config=Config(os.path.join(dir,'config.json'))\n",
    "# model=BestSelector(os.path.join(dir,'metrics.json'))\n",
    "# saveDir=r'save_weights\\BinaryClassificationMobileNetV3Large-acc=0.74336-loss=1.671334-max_epochs=40-1100'\n",
    "Model,config,logs=utils.loadProcess(saveDir=saveDir)\n",
    "metrics=train_val.validate_model(\n",
    "    model=Model.model,\n",
    "    val_loader=test_loader,\n",
    "    device=config.device,\n",
    "    only_val=True,\n",
    "    criterion=criterion\n",
    ")\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存数据到tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import metrics,utils\n",
    "recoder=metrics.TensorboardRecorder(#存到tensorboard显示\n",
    "    log_dir=\"runs/\",\n",
    "    input_shape=[4,3,128,128],\n",
    "    model=model\n",
    "\n",
    ")\n",
    "recoder.logs_scalars(\n",
    "    logs.logs,\n",
    "    prefix=\"train\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmdetection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
